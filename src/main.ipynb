{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì∞ News Category Classification\n",
    "\n",
    "This notebook tackles a multi-class classification task using news headlines and short descriptions from the HuffPost dataset.\n",
    "\n",
    "My goal is to predict the category of a news article based on its text. I explore both classical and deep learning models, including:\n",
    "\n",
    "- A Linear Support Vector Machine (SVM) using TF-IDF features and TruncatedSVD\n",
    "- Deep learning models (CNN, LSTM, GRU) using pretrained GloVe embeddings\n",
    "- Hyperparameter tuning with Keras Tuner for optimal CNN performance\n",
    "\n",
    "I preprocess the text, address category imbalance, and compare the models based on accuracy and weighted F1-score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Environment & Libraries\n",
    "\n",
    "I began by importing the necessary libraries. These include:\n",
    "- **Data handling** tools like pandas and numpy\n",
    "- **Text processing** with `nltk` and `spaCy`\n",
    "- **Machine learning** components, including `TruncatedSVD` (used instead of PCA because TF-IDF vectors are sparse), `LinearSVC`, and model evaluation metrics\n",
    "- **Deep learning** with TensorFlow and Keras\n",
    "- **Pretrained embeddings** (we'll use GloVe)\n",
    "- **Progress tracking** with tqdm\n",
    "- **Hyperparameter tuning** with Keras Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data handling and utilities\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Text preprocessing\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning: Classical models\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD  # Dimensionality reduction for sparse TF-IDF\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, LSTM, GRU\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Progress tracking and hyperparameter tuning\n",
    "from tqdm import tqdm\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set seeds for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Setup: Download NLTK & Load spaCy model to work with stopwords \n",
    "\n",
    "To lemmatize text using spaCy, I used the small English model `en_core_web_sm`.\n",
    "\n",
    "If you're running this notebook for the first time, you may need to install the model manually using the command below in your terminal or command prompt (outside of the notebook):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment these lines if you're running this for the first time\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Ingestion & Category Cleaning\n",
    "\n",
    "I use the HuffPost News Category Dataset, which is stored in JSON lines format. Each line represents a news article with fields like `headline`, `short_description`, `category`, `link`, and `date`.\n",
    "\n",
    "I begin by loading the data into a pandas DataFrame and inspecting the first few rows to understand its structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load JSON Data and Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON lines from data directory\n",
    "data = []\n",
    "with open('../data/news_category_dataset.json', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display first few rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Combine headline and short_description\n",
    "\n",
    "The dataset includes both a `headline` and a `short_description` column. I concatenate these two fields into a new column called `text`, which will be the input for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine headline and short_description into a single field\n",
    "df['text'] = df['headline'].fillna('') + ' ' + df['short_description'].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Preview Existing Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all unique categories and their frequencies\n",
    "category_counts = df['category'].value_counts()\n",
    "print(\"Total unique categories:\", len(category_counts))\n",
    "print(\"\\nCategory distribution:\\n\")\n",
    "print(category_counts)\n",
    "\n",
    "# Optional: display normalized proportions\n",
    "print(\"\\nNormalized proportions:\\n\")\n",
    "print((category_counts / len(df)).round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Merge Semantically Similar Categories\n",
    "\n",
    "The original dataset contains 42 categories, many of which are semantically redundant or too granular (e.g., `\"STYLE\"` vs. `\"STYLE & BEAUTY\"`, `\"THE WORLDPOST\"` vs. `\"WORLDPOST\"`). These overlaps create label noise and increase sparsity.\n",
    "\n",
    "I merged these categories to:\n",
    "- Reduce semantic redundancy\n",
    "- Minimize class imbalance and data sparsity\n",
    "- Improve model learnability by having fewer, more distinct target labels\n",
    "\n",
    "\n",
    "To improve model clarity and balance, I merged overlapping categories, ro reduce semantic redundancy, minimize class imbalance and data sparsity and to improve model learnability by having fewer, more distinct target labels. \n",
    "\n",
    "I also dropped very rare categories (e.g., `\"LATINO VOICES\"`), and then encoded the cleaned labels into numerical format using `LabelEncoder`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge semantically similar categories\n",
    "category_merge_map = {\n",
    "    'ARTS': 'ARTS & CULTURE',\n",
    "    'CULTURE & ARTS': 'ARTS & CULTURE',\n",
    "    'PARENTS': 'PARENTING',\n",
    "    'THE WORLDPOST': 'WORLD NEWS',\n",
    "    'WORLDPOST': 'WORLD NEWS',\n",
    "    'STYLE': 'STYLE & BEAUTY',\n",
    "    'HEALTHY LIVING': 'WELLNESS',\n",
    "    'TASTE': 'FOOD & DRINK',\n",
    "    'GREEN': 'ENVIRONMENT',\n",
    "    'COLLEGE': 'EDUCATION',\n",
    "    'FIFTY': 'WELLNESS',\n",
    "    'GOOD NEWS': 'IMPACT',\n",
    "    'U.S. NEWS': 'POLITICS'\n",
    "}\n",
    "\n",
    "# Apply the mapping\n",
    "df['category'] = df['category'].replace(category_merge_map)\n",
    "\n",
    "# Category Count\n",
    "print(df['category'].value_counts())\n",
    "print(\"New number of categories:\", df['category'].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After merging, the number of unique categories dropped from 42 to 28, making the task more tractable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Remove out-of-scope categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rare or out-of-scope categories\n",
    "drop_categories = ['LATINO VOICES']\n",
    "df = df[~df['category'].isin(drop_categories)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Encode Categories as Numeric Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have named categories (e.g., \"COMEDY\", \"U.S. NEWS\")\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['category'])  # or df['Class Index'] if CSV has numbers\n",
    "\n",
    "# View label mapping\n",
    "label_map = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(label_map)\n",
    "\n",
    "# Show updated dataframe\n",
    "df[['text', 'category', 'label']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Save the Full Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"cleaned_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Exploratory Data Analysis (EDA)\n",
    "\n",
    "I analyze the distribution of categories and article lengths to understand data characteristics and support key modeling choices. This includes:\n",
    "\n",
    "- Visualizing class imbalance across categories\n",
    "- Identifying empty or extremely short entries\n",
    "- Understanding typical text lengths (word count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Plot Class Distribution for each categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the top 15 most common categories to illustrate class imbalance\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=df, y='category', order=df['category'].value_counts().index[:15])\n",
    "plt.title(\"Top 15 Category Distribution\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Category\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print total number of categories and their frequency counts\n",
    "print(\"Number of categories:\", df['category'].nunique())\n",
    "print(\"Normalized category counts:\\n\", df['category'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Check for Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values summary\n",
    "print(\"Missing Values:\\n\", df.isnull().sum())\n",
    "\n",
    "# Distribution of empty texts\n",
    "print(\"Empty text rows:\", (df['text'].str.strip() == '').sum())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** While no `NaN` values were found, I discovered 5 rows where the `text` field was an empty string.\n",
    "These were removed from the dataset to avoid passing blank inputs into downstream models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with empty 'text' values (even if not null)\n",
    "df = df[~(df['text'].str.strip() == '')]\n",
    "\n",
    "# Distribution of empty texts\n",
    "print(\"Empty text rows:\", (df['text'].str.strip() == '').sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Analyze Text Length (Word Count)\n",
    "\n",
    "Most articles are short. I compute word counts per row and examine the distribution to support decisions like padding length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add basic length stats\n",
    "df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Summary stats\n",
    "print(\"Word Count Stats:\\n\", df['word_count'].describe())\n",
    "\n",
    "# Histogram of word counts\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.histplot(df['word_count'], bins=50, kde=True)\n",
    "plt.title(\"Distribution of Text Length (in Words)\")\n",
    "plt.xlabel(\"Word Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize the 95th percentile word count to support the 100-token sequence length\n",
    "p95 = df['word_count'].quantile(0.95)\n",
    "print(\"95th percentile of word count:\", p95)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Average Word Count by Category\n",
    "\n",
    "Certain categories (like WELLNESS or MONEY) tend to have longer texts. This justifies setting a slightly generous sequence length (e.g., 100 tokens) in the deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average length per category\n",
    "category_wc = df.groupby('category')['word_count'].mean().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "category_wc.plot(kind='barh')\n",
    "plt.title(\"Average Text Length per Category\")\n",
    "plt.xlabel(\"Average Word Count\")\n",
    "plt.ylabel(\"Category\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the EDA above:\n",
    "- The **average article text length** is around 29 words, with most falling between 20 and 35 words (and 95% of them below 55 words).\n",
    "- A few longer entries extend up to 245 words, but these are outliers.\n",
    "- Some categories (like MONEY or WELLNESS) contain slightly longer descriptions on average.\n",
    "\n",
    "To account for this, I chose:\n",
    "- `MAX_SEQUENCE_LENGTH = 100` in the deep learning models, which allows coverage for nearly all samples without excessive padding.\n",
    "- I used a **sample of 50,000 rows** for training due to compute constraints, but this can easily be scaled up to +200k in future runs.\n",
    "\n",
    "These choices were based on a balance between capturing enough context and maintaining training efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Display Example Snippets by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show 1 example per category\n",
    "example_snippets = df.groupby('category')['text'].apply(lambda x: x.sample(1).values[0])\n",
    "for cat, text in example_snippets.items():\n",
    "    print(f\"--- {cat} ---\")\n",
    "    print(text[:300], '...\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Text Preprocessing using spaCy\n",
    "\n",
    "To prepare the data for modeling, I apply the following preprocessing steps:\n",
    "\n",
    "- Lowercasing and character filtering (retain letters, numbers, `$`, and `#`)\n",
    "- Lemmatization using `spaCy`\n",
    "- Stopword and punctuation removal using `nltk` and `spaCy`\n",
    "- Removal of short and irrelevant tokens (e.g., length ‚â§ 2)\n",
    "\n",
    "Lemmatization standardizes words (e.g., ‚Äúrunning‚Äù ‚Üí ‚Äúrun‚Äù) and improves generalization. I used `spaCy`'s `nlp.pipe()` method to process texts in batches efficiently.\n",
    "\n",
    "Because processing the full dataset is computationally expensive, I used a random sample of 50,000 articles for model development.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Define a basic cleaning function to lowercase and remove non-letter characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic cleaning: lowercase text and remove non-alphanumeric characters (except $ and #)\n",
    "def clean_for_spacy(text):\n",
    "    text = text.lower()\n",
    "    # Keep words, digits, $ and # symbols\n",
    "    text = re.sub(r'[^a-z0-9$#\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Fast preprocessing using spaCy's nlp.pipe for lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply spaCy lemmatization and filter tokens\n",
    "def fast_preprocess(texts):\n",
    "    cleaned_texts = [clean_for_spacy(t) for t in texts]\n",
    "    docs = nlp.pipe(cleaned_texts, batch_size=64)\n",
    "    return [\n",
    "        ' '.join(\n",
    "            token.lemma_ for token in doc \n",
    "            if token.lemma_ not in stop_words and len(token) > 2 and not token.is_stop and not token.is_punct\n",
    "        )\n",
    "        for doc in docs\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Choose Dataset Version\n",
    "\n",
    "To speed up experimentation, I randomly sample 50,000 rows from the cleaned dataset. This sample maintains class proportions due to stratified splitting later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Process the full dataset (this is very slow, it took me 8 hours to run...)\n",
    "# df['clean_text'] = fast_preprocess(df['text'])\n",
    "\n",
    "# Option 2: Use a 50k-row sample for faster processing\n",
    "df_sample = df.sample(n=50000, random_state=42).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Enhanced preprocessing with tqdm progress tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply spaCy lemmatization and filter tokens\n",
    "def fast_preprocess_with_tqdm(texts):\n",
    "    cleaned_texts = [clean_for_spacy(t) for t in texts]\n",
    "    docs = list(tqdm(nlp.pipe(cleaned_texts, batch_size=64), total=len(cleaned_texts)))\n",
    "    results = []\n",
    "    for doc in docs:\n",
    "        tokens = [\n",
    "            token.lemma_ for token in doc\n",
    "            if len(token) > 2 and token.is_alpha and not token.is_stop and token.lemma_ not in stop_words\n",
    "        ]\n",
    "        results.append(' '.join(tokens))\n",
    "    return results\n",
    "\n",
    "# Apply cleaning and lemmatization\n",
    "df_sample['clean_text'] = fast_preprocess_with_tqdm(df_sample['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Save the preprocessed sample for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for reuse\n",
    "df_sample.to_csv(\"cleaned_sample_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Check Most Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all tokens\n",
    "all_tokens = ' '.join(df_sample['clean_text']).split()\n",
    "common_words = Counter(all_tokens).most_common(20)\n",
    "\n",
    "# Display\n",
    "print(\"Most common words:\", common_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Data Splitting (Train/Validation/Test)\n",
    "\n",
    "To evaluate models fairly, I split the dataset into three sets:\n",
    "\n",
    "- **70% for training**: Used to fit the model\n",
    "- **15% for validation**: Used to tune hyperparameters and monitor overfitting\n",
    "- **15% for testing**: Held out for final evaluation\n",
    "\n",
    "I also used **stratified sampling** to preserve the original category distribution across all splits. This is important for multi-class classification tasks with imbalance (e.g., POLITICS has far more samples than MONEY or SCIENCE).\n",
    "\n",
    "Although I experimented with other splits (like 80/10/10), I found 70/15/15 to provide a more reliable validation set without sacrificing too much training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 0: Use the sample dataset for subsequent modeling\n",
    "sample_df = df_sample.copy()  # contains 'text', 'label', and 'clean_text'\n",
    "\n",
    "# Step 1: Split into train (70%) and temp (30%)\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "    sample_df['text'], sample_df['label'], \n",
    "    test_size=0.30, \n",
    "    stratify=sample_df['label'], \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Step 2: Split temp into validation (15%) and test (15%)\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    temp_texts, temp_labels, \n",
    "    test_size=0.50, \n",
    "    stratify=temp_labels, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Step 3: Recover full rows for each split\n",
    "# Building DataFrames by locating rows using boolean indexing\n",
    "train_df = sample_df[sample_df['text'].isin(train_texts)].copy()\n",
    "val_df = sample_df[sample_df['text'].isin(val_texts)].copy()\n",
    "test_df = sample_df[sample_df['text'].isin(test_texts)].copy()\n",
    "\n",
    "# Step 4: Confirm proportions\n",
    "print(f\"Training samples: {len(train_texts)}\")\n",
    "print(f\"Validation samples: {len(val_texts)}\")\n",
    "print(f\"Test samples: {len(test_texts)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Baseline Model ‚Äì Linear SVM (TF-IDF + TruncatedSVD)\n",
    "\n",
    "As a classical baseline, I trained a **Linear SVM** using **TF-IDF features**. Since TF-IDF vectors are sparse and high-dimensional, I applied **TruncatedSVD** (a dimensionality reduction method suited for sparse matrices) to project the features into 300 latent components.\n",
    "\n",
    "This model is useful for benchmarking because it is fast to train, interpretable, and often strong for text tasks. I also used `class_weight='balanced'` to offset category imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),      # Use unigrams and bigrams\n",
    "    max_features=10000,      # Limit vocabulary size\n",
    "    min_df=5,                # Ignore rare words\n",
    "    max_df=0.8,              # Ignore overly common words\n",
    ")\n",
    "\n",
    "# Fit on training set only to avoid data leakage\n",
    "X_train_tfidf = tfidf.fit_transform(train_df['clean_text'])\n",
    "X_val_tfidf = tfidf.transform(val_df['clean_text'])\n",
    "X_test_tfidf = tfidf.transform(test_df['clean_text'])\n",
    "\n",
    "# Labels\n",
    "y_train = train_df['label']\n",
    "y_val = val_df['label']\n",
    "y_test = test_df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Dimensionality Reduction with SVD on TF-IDF Vectors \n",
    "\n",
    "Since TF-IDF matrices are sparse, I used TruncatedSVD (not PCA) to reduce feature dimensions from 10,000 to 300.\n",
    "\n",
    "This technique, also known as Latent Semantic Analysis (LSA), preserves semantic relationships while improving speed and memory usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply TruncatedSVD to reduce dimensionality of sparse TF-IDF vectors\n",
    "svd = TruncatedSVD(n_components=300, random_state=42)\n",
    "\n",
    "X_train_svd = svd.fit_transform(X_train_tfidf)\n",
    "X_val_svd = svd.transform(X_val_tfidf)\n",
    "X_test_svd = svd.transform(X_test_tfidf)\n",
    "\n",
    "# Check how much variance we're keeping\n",
    "print(\"Explained variance ratio (sum):\", svd.explained_variance_ratio_.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Train a Linear SVM with GridSearchCV\n",
    "\n",
    "I used `GridSearchCV` to tune the `C` hyperparameter across a wide range. The scoring metric was **weighted F1-score**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM model with class_weight, to handle class imbalance\n",
    "svm = LinearSVC(max_iter=5000, class_weight='balanced')\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Grid Search on validation split\n",
    "grid = GridSearchCV(\n",
    "    svm,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='f1_weighted',\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid.fit(X_train_svd, y_train)\n",
    "\n",
    "# Best model\n",
    "best_svm = grid.best_estimator_\n",
    "print(\"Best C:\", grid.best_params_['C'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Evaluate the SVM Model on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_pred = best_svm.predict(X_test_svd)\n",
    "\n",
    "# Metrics\n",
    "print(\"SVM Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"SVM Test F1 Score (weighted):\", f1_score(y_test, y_pred, average='weighted'))\n",
    "print(\"\\nSVM Classification Report:\\n\", classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), \n",
    "            annot=False, \n",
    "            cmap=\"Blues\", \n",
    "            xticklabels=label_encoder.classes_, \n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Deep Learning Model ‚Äì CNN for Text Classification\n",
    "\n",
    "I implemented a 1D Convolutional Neural Network (CNN), which performs well on short texts. CNNs extract local patterns (e.g., ‚Äúclimate change‚Äù, ‚Äúbreaking news‚Äù) and are less resource-intensive than RNNs like LSTM or GRU.\n",
    "\n",
    "I used **GloVe (Global Vectors for Word Representation)** as pretrained word embeddings. These capture semantic relationships between words. By setting `trainable=True`, the model fine-tunes the embeddings to better fit the HuffPost news domain.\n",
    "\n",
    "The architecture consists of:\n",
    "- An Embedding layer initialized with GloVe vectors\n",
    "- A Conv1D layer to learn local patterns\n",
    "- Global max pooling to reduce sequence length\n",
    "- A dense layer + dropout for transformation and regularization\n",
    "- A final softmax output layer for classification\n",
    "\n",
    "\n",
    "**Note:** For all deep learning models, I now used the `clean_text` column, which includes lowercasing, character filtering, lemmatization, and stopword removal using spaCy. This ensures consistency across our pipeline and improves signal-to-noise ratio. The same applies to the following LSTM and GRU models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Tokenization & Sequence Preparation\n",
    "\n",
    "We use the `Tokenizer` from Keras to convert text into integer sequences, limiting the vocabulary to 20,000 words and padding all sequences to a max length of 100 tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "MAX_NUM_WORDS = 20000         # max vocabulary size\n",
    "MAX_SEQUENCE_LENGTH = 100     # max length of padded sequences\n",
    "\n",
    "# Tokenize and convert to sequences using cleaned, lemmatized text\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train_df['clean_text'])\n",
    "\n",
    "# Convert texts to padded sequences\n",
    "X_train_seq = pad_sequences(tokenizer.texts_to_sequences(train_df['clean_text']), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_val_seq = pad_sequences(tokenizer.texts_to_sequences(val_df['clean_text']), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(test_df['clean_text']), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Labels for deep learning\n",
    "y_train_cat = tf.keras.utils.to_categorical(train_df['label'])\n",
    "y_val_cat = tf.keras.utils.to_categorical(val_df['label'])\n",
    "y_test_cat = tf.keras.utils.to_categorical(test_df['label'])\n",
    "\n",
    "# Vocabulary and class dimensions\n",
    "vocab_size = min(MAX_NUM_WORDS, len(tokenizer.word_index) + 1)\n",
    "num_classes = y_train_cat.shape[1]\n",
    "\n",
    "# Compute class weights for imbalance\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_df['label']),\n",
    "    y=train_df['label']\n",
    ")\n",
    "class_weights_dict = dict(enumerate(class_weights))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Load Pretrained GloVe Embeddings\n",
    "\n",
    "I used GloVe 100-dimensional vectors to initialize the embedding layer. For each word in my vocabulary, I retrieved its GloVe vector (if available). Unknown tokens are set to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe file (download e.g. glove.6B.100d.txt from https://nlp.stanford.edu/projects/glove/)\n",
    "embedding_dim = 100\n",
    "embedding_index = {}\n",
    "\n",
    "with open('../data/glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coeffs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coeffs\n",
    "\n",
    "# Build embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < vocab_size:\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Define and Compile the CNN Model\n",
    "\n",
    "I set `trainable=True` in the embedding layer to fine-tune the word vectors during training, which often helps adapt them better to the specific vocabulary of this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I built a 1D Convolutional Neural Network (CNN) for text classification.\n",
    "# The architecture is designed to capture local n-gram patterns from sequences of tokens.\n",
    "\n",
    "cnn_model = Sequential()\n",
    "\n",
    "# I initialized the embedding layer using pretrained GloVe vectors\n",
    "# and set trainable=True to allow fine-tuning on my dataset.\n",
    "cnn_model.add(Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=embedding_dim,\n",
    "    weights=[embedding_matrix],\n",
    "    trainable=True\n",
    "))\n",
    "\n",
    "# A Conv1D layer helps detect local features such as phrases and sentiment cues.\n",
    "cnn_model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "\n",
    "# Global max pooling reduces the sequence dimension and keeps the strongest signal.\n",
    "cnn_model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# A dense layer to increase model capacity and non-linearity.\n",
    "cnn_model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Dropout helps prevent overfitting.\n",
    "cnn_model.add(Dropout(0.5))\n",
    "\n",
    "# Final softmax layer to output probabilities across all 28 categories.\n",
    "cnn_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "cnn_model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# Summary of architecture\n",
    "cnn_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Train the CNN Model with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping callback\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Fit\n",
    "history = cnn_model.fit(\n",
    "    X_train_seq, y_train_cat,\n",
    "    validation_data=(X_val_seq, y_val_cat),\n",
    "    epochs=15,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    class_weight=class_weights_dict  # to balance the labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Evaluate CNN Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training/validation loss\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title(\"CNN Model Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_cnn = cnn_model.predict(X_test_seq).argmax(axis=1)\n",
    "# Evaluation\n",
    "print(\"CNN Classification Report:\\n\", \n",
    "      classification_report(test_df['label'], y_pred_cnn, target_names=label_encoder.classes_))\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "sns.heatmap(confusion_matrix(test_df['label'], y_pred_cnn), \n",
    "            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_,\n",
    "            annot=False, cmap=\"Purples\")\n",
    "plt.title(\"CNN Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Additional RNN Models ‚Äì LSTM and GRU\n",
    "\n",
    "To compare performance across architectures, I also implemented LSTM and GRU models. These models are designed to capture sequential dependencies in longer texts, but given the short input sequences in this dataset, I expected limited benefit over CNN.\n",
    "\n",
    "Both models reuse the same GloVe-initialized embedding layer and input sequences (`X_train_seq`, `X_val_seq`, `X_test_seq`). Early stopping is used to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM-based model\n",
    "lstm_model = Sequential()\n",
    "\n",
    "lstm_model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim,\n",
    "                         weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH,\n",
    "                         trainable=True))\n",
    "lstm_model.add(LSTM(64, return_sequences=False))\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(Dense(64, activation='relu'))\n",
    "lstm_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "lstm_model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "lstm_model.summary()\n",
    "\n",
    "# Train with early stopping and Evaluate (same as CNN and LSTM)\n",
    "history_lstm = lstm_model.fit(\n",
    "    X_train_seq, y_train_cat,\n",
    "    validation_data=(X_val_seq, y_val_cat),\n",
    "    epochs=15,\n",
    "    batch_size=32,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "# Generate prediction for LSTM model\n",
    "y_pred_lstm = lstm_model.predict(X_test_seq).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GRU-based model\n",
    "gru_model = Sequential()\n",
    "\n",
    "gru_model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim,\n",
    "                        weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH,\n",
    "                        trainable=True))\n",
    "gru_model.add(GRU(64))\n",
    "gru_model.add(Dropout(0.5))\n",
    "gru_model.add(Dense(64, activation='relu'))\n",
    "gru_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "gru_model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "gru_model.summary()\n",
    "\n",
    "# Train with early stopping and Evaluate (same as CNN and LSTM)\n",
    "history_gru = gru_model.fit(\n",
    "    X_train_seq, y_train_cat,\n",
    "    validation_data=(X_val_seq, y_val_cat),\n",
    "    epochs=15,\n",
    "    batch_size=32,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "# Generate prediction for LSTM model\n",
    "y_pred_gru = gru_model.predict(X_test_seq).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Hyperparameter Tuning with Keras Tuner \n",
    "\n",
    "To improve the CNN performance, I used **Keras Tuner** to explore different architectural configurations.\n",
    "The goal was to maximize validation accuracy by adjusting:\n",
    "\n",
    "- **Embedding Dimension** (50, 100, 200): Higher dimensions may capture more semantics, but take more memory.\n",
    "- **Number of Filters** in Conv1D (64‚Äì256): More filters can extract richer features from the text.\n",
    "- **Kernel Size** (3‚Äì5): Controls the size of the n-gram pattern captured (e.g., trigrams vs. pentagrams).\n",
    "- **Dropout Rate** (0.3‚Äì0.6): Helps prevent overfitting by randomly deactivating neurons.\n",
    "- **Dense Layer Size** (32‚Äì128): Controls the complexity of the model before classification.\n",
    "\n",
    "\n",
    "\n",
    "The objective was to **maximize validation accuracy** on the 15% held-out validation set. I ran 10 trials and selected the best model based on validation accuracy, and I used `RandomSearch` with early stopping for efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Define Tuner Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Install Keras Tuner (if not already)\n",
    "# pip install keras-tuner --quiet \n",
    "\n",
    "# I used Keras Tuner to optimize the CNN architecture.\n",
    "# The search space included embedding size, number of filters, kernel size, dropout rate, and dense layer size.\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Search for optimal embedding dimension\n",
    "    model.add(Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=hp.Choice('embedding_dim', [50, 100, 200]),  # 100 worked best\n",
    "        trainable=True\n",
    "    ))\n",
    "\n",
    "    # Tune the number of filters and kernel size for the Conv1D layer\n",
    "    model.add(Conv1D(\n",
    "        filters=hp.Int('filters', min_value=64, max_value=256, step=64),  # controls feature richness\n",
    "        kernel_size=hp.Choice('kernel_size', [3, 4, 5]),  # controls local context window\n",
    "        activation='relu'\n",
    "    ))\n",
    "\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "\n",
    "    # Dropout to control overfitting\n",
    "    model.add(Dropout(hp.Float('dropout', 0.3, 0.6, step=0.1)))\n",
    "\n",
    "    # Dense layer to boost model capacity\n",
    "    model.add(Dense(units=hp.Int('dense_units', 32, 128, step=32), activation='relu'))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Run the Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='tuner_results',\n",
    "    project_name='cnn_text_classification_final_version'\n",
    ")\n",
    "\n",
    "tuner.search(X_train_seq, y_train_cat,\n",
    "             epochs=10,\n",
    "             validation_data=(X_val_seq, y_val_cat),\n",
    "             callbacks=[EarlyStopping(monitor='val_loss', patience=2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Get Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "print(\"Best Hyperparameters:\", best_hp.values)\n",
    "\n",
    "best_model_tuned = tuner.get_best_models(1)[0]\n",
    "best_model_tuned.evaluate(X_test_seq, y_test_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Best Architecture\n",
    "\n",
    "The best CNN configuration had:\n",
    "- Embedding dimension = 100\n",
    "- 256 convolution filters\n",
    "- Kernel size = 3\n",
    "- Dropout rate = 0.5\n",
    "- Dense units = 32\n",
    "\n",
    "This suggests that a relatively **compact but expressive model** performed better than larger alternatives, possibly due to overfitting on such a limited training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Final Model Evaluation & Comparison\n",
    "\n",
    "After training and tuning all models, I compared their performance using accuracy and weighted F1-score on the test set.\n",
    "\n",
    "The CNN tuned with Keras Tuner achieved the best performance, followed by the GRU and LSTM models. The SVM baseline performed reasonably well given its simplicity but was outperformed by all deep learning models.\n",
    "\n",
    "This comparison helps highlight the trade-offs between speed, accuracy, and complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Evaluate and compare the SVM and tuned CNN models\n",
    "\n",
    "I formatted the `classification_report` as a pandas DataFrame to sort and display class-wise F1-scores more clearly. This allows for easier identification of strong and weak categories.\n",
    "\n",
    "I also included a bar chart to highlight which categories were classified most accurately by the CNN model (e.g., WELLNESS, STYLE & BEAUTY), and which ones were more difficult (e.g., MONEY, SCIENCE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final predictions (already computed earlier)\n",
    "y_pred_svm = best_svm.predict(X_test_svd)\n",
    "y_pred_cnn_tuned = best_model_tuned.predict(X_test_seq).argmax(axis=1)\n",
    "y_true = test_df['label']\n",
    "\n",
    "# Print metrics\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_true, y_pred_svm))\n",
    "print(\"CNN (Tuned) Accuracy:\", accuracy_score(y_true, y_pred_cnn_tuned))\n",
    "\n",
    "print(\"\\nSVM Classification Report:\\n\", classification_report(y_true, y_pred_svm, target_names=label_encoder.classes_))\n",
    "print(\"\\nCNN (Tuned) Classification Report:\\n\", classification_report(y_true, y_pred_cnn_tuned, target_names=label_encoder.classes_))\n",
    "\n",
    "\n",
    "# Tabular classification report for CNN (tuned)\n",
    "report_dict = classification_report(y_true, y_pred_cnn_tuned, target_names=label_encoder.classes_, output_dict=True)\n",
    "report_df = pd.DataFrame(report_dict).transpose()\n",
    "report_df = report_df.sort_values(by='f1-score', ascending=False)\n",
    "\n",
    "# Preview top-performing classes\n",
    "report_df.head(10)\n",
    "\n",
    "\n",
    "# Drop macro/weighted rows and plot F1 score by class\n",
    "report_df_filtered = report_df.drop(['accuracy', 'macro avg', 'weighted avg'])\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=report_df_filtered['f1-score'], y=report_df_filtered.index)\n",
    "plt.title(\"F1 Score by Category ‚Äì CNN (Tuned)\")\n",
    "plt.xlabel(\"F1 Score\")\n",
    "plt.ylabel(\"Category\")\n",
    "plt.xlim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Confusion matrix (CNN)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_matrix(y_true, y_pred_cnn_tuned),\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_,\n",
    "            cmap=\"Blues\", annot=False)\n",
    "plt.title(\"CNN (Tuned) Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "- The tuned CNN achieved the highest performance, validating the importance of architecture tuning.\n",
    "- GRU slightly outperformed LSTM, while both offered modest gains over the untuned CNN.\n",
    "- The SVM was faster to train and easier to interpret, but its performance lagged behind deep learning models.\n",
    "- All models benefited from careful preprocessing, class merging, and stratified sampling.\n",
    "\n",
    "In future work, I would consider using transformer-based models like BERT, leveraging hierarchical classification, and scaling training to the full 200k+ dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Display some misclassified examples from the CNN model\n",
    "\n",
    "To better understand where the model struggles, I display a few misclassified test examples from the best-performing CNN model.\n",
    "\n",
    "Common issues include:\n",
    "- Overlap between categories (e.g., *STYLE & BEAUTY* vs. *ENTERTAINMENT*)\n",
    "- Headlines that lack context or contain ambiguous language\n",
    "- Samples that could reasonably belong to more than one class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some wrong predictions\n",
    "misclassified = test_df.copy()\n",
    "misclassified['pred'] = y_pred_cnn_tuned\n",
    "\n",
    "# Filter incorrect predictions\n",
    "wrong = misclassified[misclassified['label'] != misclassified['pred']]\n",
    "\n",
    "# Decode to category names\n",
    "wrong['true_cat'] = label_encoder.inverse_transform(wrong['label'])\n",
    "wrong['pred_cat'] = label_encoder.inverse_transform(wrong['pred'])\n",
    "\n",
    "# Show examples\n",
    "for i in range(min(5, len(wrong))):\n",
    "    print(f\"\\n--- Misclassified Example {i+1} ---\")\n",
    "    print(\"Text:\", wrong.iloc[i]['text'][:300], \"...\")\n",
    "    print(\"True Label:\", wrong.iloc[i]['true_cat'])\n",
    "    print(\"Predicted Label:\", wrong.iloc[i]['pred_cat'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Some common sources of error:\n",
    "- Headlines or short descriptions that lack specific keywords (e.g., \"Stop the Presses!\")\n",
    "- Topics that naturally overlap (e.g., *WELLNESS* and *WOMEN*, or *MEDIA* and *COMEDY*)\n",
    "- Sarcastic or informal language, which can be harder to interpret without context\n",
    "\n",
    "This suggests future improvements could include using full article content, applying attention mechanisms, or integrating transformer models like BERT.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Final Model Comparison\n",
    "\n",
    "To evaluate performance, I compared four models on the same test set:\n",
    "\n",
    "- **SVM (TF-IDF + TruncatedSVD)**: A strong linear baseline that‚Äôs often competitive for text tasks.\n",
    "- **CNN (Tuned)**: A deep learning model capable of capturing local patterns in text (e.g., phrase-level features).\n",
    "- **LSTM**: A recurrent model that processes tokens sequentially and is capable of modeling context.\n",
    "- **GRU**: A lighter, faster alternative to LSTM with fewer parameters.\n",
    "\n",
    "The table below summarizes their performance on accuracy and weighted F1 score:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final performance summary\n",
    "summary = pd.DataFrame({\n",
    "    \"Model\": [\n",
    "        \"SVM (TF-IDF + TruncatedSVD)\", \n",
    "        \"CNN (Tuned)\", \n",
    "        \"LSTM\", \n",
    "        \"GRU\"\n",
    "    ],\n",
    "    \"Accuracy\": [\n",
    "        accuracy_score(y_true, y_pred_svm),\n",
    "        accuracy_score(y_true, y_pred_cnn_tuned),\n",
    "        accuracy_score(y_true, y_pred_lstm),\n",
    "        accuracy_score(y_true, y_pred_gru)\n",
    "    ],\n",
    "    \"F1 Score (weighted)\": [\n",
    "        f1_score(y_true, y_pred_svm, average='weighted'),\n",
    "        f1_score(y_true, y_pred_cnn_tuned, average='weighted'),\n",
    "        f1_score(y_true, y_pred_lstm, average='weighted'),\n",
    "        f1_score(y_true, y_pred_gru, average='weighted')\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Bar chart comparison \n",
    "summary.plot(x='Model', y=['Accuracy', 'F1 Score (weighted)'], kind='bar', figsize=(10, 6))\n",
    "plt.title(\"Model Performance Comparison\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(rotation=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nModel Performance Summary:\\n\", summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the **LSTM and GRU models performed reasonably well**, they required significantly longer training times and only marginally improved classification for a few categories.\n",
    "\n",
    "For this reason, I decided to focus on **CNN vs. SVM** for the final discussion:\n",
    "- **SVM** offers fast training and interpretability but struggles to capture deeper semantic patterns.\n",
    "- **CNN** is more expressive and outperformed SVM in both accuracy and F1 score after tuning.\n",
    "\n",
    "Given the short and structured nature of the news headlines, a 1-layer CNN struck a good balance between performance and efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This project demonstrated the performance of traditional and deep learning approaches to multi-class text classification.\n",
    "\n",
    "Key takeaways:\n",
    "- **Data preprocessing** and **category merging** significantly reduced class fragmentation and improved model clarity.\n",
    "- A **Linear SVM** with TF-IDF + TruncatedSVD provided a strong, interpretable baseline.\n",
    "- A **tuned CNN** outperformed all other models while remaining computationally efficient.\n",
    "- **LSTM and GRU** models were explored, but offered limited gains compared to CNN under time and hardware constraints.\n",
    "\n",
    "Future improvements could include:\n",
    "- Testing **transformer-based architectures** like BERT\n",
    "- Leveraging more of the original dataset (up to 100k+ rows)\n",
    "- Using **ensembles** or **attention mechanisms** for performance boosts\n",
    "\n",
    "Overall, this was a valuable exercise in balancing model performance, interpretability, and efficiency.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
